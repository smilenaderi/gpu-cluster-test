#!/bin/bash
#
# GPU Cluster Test - Universal CLI Tool
#
# A unified command-line interface for GPU cluster validation.
# All output is displayed in real-time - no need to check log files.
#
# Usage: ./gpu-test <command> [options]
#
# Commands:
#   validate    Run cluster validation test (distributed training)
#   nccl        Run NCCL communication test
#   diagnose    Run comprehensive GPU diagnostics
#   import      Import custom Docker image from GHCR
#   help        Show this help message
#
# Options:
#   --nodes N              Number of nodes (default: 2)
#   --gpus-per-node N      GPUs per node (default: 8)
#   --epochs N             Training epochs (default: 5)
#   --batch-size N         Batch size per GPU (default: 64)
#   --interactive          Run interactively (Slurm only)
#   --dry-run              Test on CPU without GPU
#   --debug                Enable full debug logging
#
# Examples:
#   ./gpu-test validate --nodes 2 --gpus-per-node 2
#   ./gpu-test nccl --nodes 4 --gpus-per-node 4
#   ./gpu-test diagnose --nodes 2 --gpus-per-node 2
#   ./gpu-test validate --nodes 1 --gpus-per-node 1 --interactive
#   ./gpu-test validate --dry-run
#

set -euo pipefail

# Get script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# Colors disabled for better compatibility
RED=''
GREEN=''
YELLOW=''
BLUE=''
NC=''

# Default configuration
COMMAND=""
NODES=2
GPUS_PER_NODE=8
EPOCHS=5
BATCH_SIZE=64
INTERACTIVE=false
DRY_RUN=false
DEBUG=false

# Parse command
if [ $# -eq 0 ]; then
    COMMAND="help"
else
    COMMAND="$1"
    shift
fi

# Parse options
while [[ $# -gt 0 ]]; do
    case $1 in
        --nodes)
            NODES="$2"
            shift 2
            ;;
        --gpus-per-node)
            GPUS_PER_NODE="$2"
            shift 2
            ;;
        --epochs)
            EPOCHS="$2"
            shift 2
            ;;
        --batch-size)
            BATCH_SIZE="$2"
            shift 2
            ;;
        -i|--interactive)
            INTERACTIVE=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --debug)
            DEBUG=true
            shift
            ;;
        --help|-h)
            COMMAND="help"
            shift
            ;;
        *)
            echo "Error: Unknown option: $1"
            echo "Use './gpu-test help' for usage information"
            exit 1
            ;;
    esac
done

# Help function
show_help() {
    cat << EOF
================================================================================
GPU Cluster Test - Universal CLI Tool
================================================================================

A unified command-line interface for GPU cluster validation.
All output is displayed in real-time - no need to check log files.

USAGE:
  ./gpu-test <command> [options]

COMMANDS:
  validate    Run cluster validation test (distributed training)
  nccl        Run NCCL communication test
  diagnose    Run comprehensive GPU diagnostics
  import      Import custom Docker image from GHCR
  help        Show this help message

OPTIONS:
  --nodes N              Number of nodes (default: 2)
  --gpus-per-node N      GPUs per node (default: 8)
  --epochs N             Training epochs (default: 5)
  --batch-size N         Batch size per GPU (default: 64)
  -i, --interactive      Run interactively with real-time output (validate only)
  --dry-run              Test on CPU without GPU
  --debug                Enable full debug logging (NCCL, PyTorch, CUDA)

EXAMPLES:
  # Small cluster test (4 GPUs)
  ./gpu-test validate --nodes 2 --gpus-per-node 2

  # Medium cluster test (16 GPUs)
  ./gpu-test validate --nodes 4 --gpus-per-node 4 --epochs 10

  # NCCL communication test
  ./gpu-test nccl --nodes 2 --gpus-per-node 2

  # Run diagnostics (single node)
  ./gpu-test diagnose

  # Run diagnostics (multi-node)
  ./gpu-test diagnose --nodes 2 --gpus-per-node 2

  # Interactive mode (see output in real-time)
  ./gpu-test validate --nodes 2 --gpus-per-node 2 -i

  # Debug mode (full logging)
  ./gpu-test validate --nodes 2 --gpus-per-node 2 --debug

  # CPU dry-run (no GPU needed)
  ./gpu-test validate --dry-run

  # Import custom Docker image
  ./gpu-test import

ENVIRONMENT DETECTION:
  The tool automatically detects your environment:
  - Slurm: Uses srun/sbatch
  - Kubernetes: Uses kubectl/PyTorchJob
  - Standalone: Uses torchrun directly

OUTPUT:
  All output is displayed in real-time. No need to check log files.
  For batch jobs, logs are also saved to: logs/<test>_<job_id>.out

================================================================================
EOF
}

# Detect environment
detect_environment() {
    if command -v sbatch &> /dev/null; then
        echo "slurm"
    elif command -v kubectl &> /dev/null && kubectl cluster-info &> /dev/null 2>&1; then
        echo "kubernetes"
    else
        echo "standalone"
    fi
}

# Run validation test
run_validate() {
    local env=$(detect_environment)
    local total_gpus=$((NODES * GPUS_PER_NODE))
    
    # Enable debug logging if requested
    if [ "$DEBUG" = true ]; then
        export NCCL_DEBUG=INFO
        export NCCL_DEBUG_SUBSYS=ALL
        export TORCH_DISTRIBUTED_DEBUG=DETAIL
        export TORCH_CPP_LOG_LEVEL=INFO
        export CUDA_LAUNCH_BLOCKING=1
        echo "üîç Debug logging enabled:"
        echo "   NCCL_DEBUG=INFO"
        echo "   NCCL_DEBUG_SUBSYS=ALL"
        echo "   TORCH_DISTRIBUTED_DEBUG=DETAIL"
        echo "   CUDA_LAUNCH_BLOCKING=1"
        echo ""
    fi
    
    echo "=========================================="
    echo "GPU Cluster Validation Test"
    echo "=========================================="
    echo "Configuration:"
    echo "  - Nodes: $NODES"
    echo "  - GPUs per node: $GPUS_PER_NODE"
    echo "  - Total GPUs: $total_gpus"
    echo "  - Epochs: $EPOCHS"
    echo "  - Batch size: $BATCH_SIZE"
    echo "  - Environment: $env"
    echo "  - Mode: $([ "$INTERACTIVE" = true ] && echo "Interactive" || echo "Batch")"
    echo "  - Debug: $([ "$DEBUG" = true ] && echo "Enabled" || echo "Disabled")"
    echo "=========================================="
    echo ""
    
    if [ "$DRY_RUN" = true ]; then
        echo "Running CPU dry-run (no GPU required)..."
        python src/train.py --dry-run --epochs "$EPOCHS"
        return $?
    fi
    
    case $env in
        slurm)
            if [ "$INTERACTIVE" = true ]; then
                echo "Running interactively with srun..."
                echo ""
                ./scripts/interactive_training_test.sh \
                    --nodes "$NODES" \
                    --gpus-per-node "$GPUS_PER_NODE" \
                    --epochs "$EPOCHS" \
                    --batch-size "$BATCH_SIZE"
            else
                echo "Submitting batch job with sbatch..."
                echo ""
                NODES=$NODES GPUS_PER_NODE=$GPUS_PER_NODE EPOCHS=$EPOCHS BATCH_SIZE=$BATCH_SIZE \
                    sbatch --nodes="$NODES" --gpus-per-node="$GPUS_PER_NODE" scripts/distributed_training_test.sh
                
                echo ""
                echo "Job submitted. Monitor with:"
                echo "  squeue -u \$USER"
                echo "  tail -f logs/acceptance_*.out"
            fi
            ;;
        kubernetes)
            echo "Deploying to Kubernetes..."
            echo ""
            kubectl apply -f kubernetes-example.yaml
            echo ""
            echo "Monitor with:"
            echo "  kubectl get pytorchjobs"
            echo "  kubectl logs -f <pod-name>"
            ;;
        standalone)
            echo "Running with torchrun..."
            echo ""
            torchrun \
                --nnodes="$NODES" \
                --nproc_per_node="$GPUS_PER_NODE" \
                src/train.py --epochs "$EPOCHS" --batch-size "$BATCH_SIZE"
            ;;
    esac
}

# Run NCCL test
run_nccl() {
    local env=$(detect_environment)
    local total_gpus=$((NODES * GPUS_PER_NODE))
    
    # Enable debug logging if requested
    if [ "$DEBUG" = true ]; then
        export NCCL_DEBUG=INFO
        export NCCL_DEBUG_SUBSYS=ALL
        export TORCH_DISTRIBUTED_DEBUG=DETAIL
        export TORCH_CPP_LOG_LEVEL=INFO
        echo "üîç Debug logging enabled:"
        echo "   NCCL_DEBUG=INFO"
        echo "   NCCL_DEBUG_SUBSYS=ALL"
        echo "   TORCH_DISTRIBUTED_DEBUG=DETAIL"
        echo ""
    fi
    
    echo "=========================================="
    echo "NCCL Communication Test"
    echo "=========================================="
    echo "Configuration:"
    echo "  - Nodes: $NODES"
    echo "  - GPUs per node: $GPUS_PER_NODE"
    echo "  - Total GPUs: $total_gpus"
    echo "  - Environment: $env"
    echo "  - Mode: $([ "$INTERACTIVE" = true ] && echo "Interactive" || echo "Batch")"
    echo "  - Debug: $([ "$DEBUG" = true ] && echo "Enabled" || echo "Disabled")"
    echo "=========================================="
    echo ""
    
    case $env in
        slurm)
            if [ "$INTERACTIVE" = true ]; then
                echo "Note: Interactive mode for NCCL test is not recommended for multi-node."
                echo "      Use batch mode instead: ./gpu-test nccl --nodes $NODES --gpus-per-node $GPUS_PER_NODE"
                echo ""
                echo "Submitting as batch job..."
                echo ""
            fi
            
            # Always use batch mode for NCCL multi-node tests
            NODES=$NODES GPUS_PER_NODE=$GPUS_PER_NODE \
                sbatch --nodes="$NODES" --gpus-per-node="$GPUS_PER_NODE" scripts/nccl_test.sh
            
            echo ""
            echo "Job submitted. Monitor with:"
            echo "  squeue -u \$USER"
            echo "  tail -f logs/nccl_*.out"
            ;;
        kubernetes)
            echo "Note: NCCL test for Kubernetes not yet implemented"
            echo "Use: kubectl apply -f kubernetes-example.yaml"
            ;;
        standalone)
            echo "Running with torchrun..."
            echo ""
            torchrun \
                --nnodes="$NODES" \
                --nproc_per_node="$GPUS_PER_NODE" \
                src/nccl_test.py
            ;;
    esac
}

# Run import
run_import() {
    echo "=========================================="
    echo "Import Custom Docker Image"
    echo "=========================================="
    echo "Importing image from GHCR..."
    echo ""
    
    if [ ! -f "scripts/import_image.sh" ]; then
        echo "Error: scripts/import_image.sh not found"
        exit 1
    fi
    
    ./scripts/import_image.sh
}

# Run diagnostics
run_diagnose() {
    local env=$(detect_environment)
    
    echo "=========================================="
    echo "GPU Cluster Diagnostics"
    echo "=========================================="
    echo "Configuration:"
    echo "  - Nodes: $NODES"
    echo "  - GPUs per node: $GPUS_PER_NODE"
    echo "  - Environment: $env"
    echo "=========================================="
    echo ""
    
    if [ "$NODES" -eq 1 ] && [ "$GPUS_PER_NODE" -eq 1 ]; then
        # Single GPU diagnostics
        echo "Running single-node diagnostics..."
        echo ""
        python src/gpu_diagnostics.py
    elif [ "$NODES" -eq 1 ]; then
        # Multi-GPU single node
        echo "Running multi-GPU diagnostics on single node..."
        echo ""
        torchrun --nproc_per_node="$GPUS_PER_NODE" src/gpu_diagnostics.py
    else
        # Multi-node diagnostics
        case $env in
            slurm)
                echo "Running multi-node diagnostics via Slurm..."
                echo ""
                echo "Creating temporary diagnostic job..."
                
                # Create temporary job script
                cat > /tmp/diagnose_job_$$.sh << 'EOFDIAG'
#!/bin/bash
#SBATCH --job-name=gpu-diagnose
#SBATCH --output=logs/diagnose_%j.out
#SBATCH --error=logs/diagnose_%j.err

echo "=========================================="
echo "Multi-Node GPU Diagnostics"
echo "=========================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Nodes: $SLURM_JOB_NUM_NODES"
echo "=========================================="
echo ""

# Get master node
MASTER_IP=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)

# Run diagnostics with torchrun
srun python src/gpu_diagnostics.py

echo ""
echo "=========================================="
echo "Diagnostics complete!"
echo "=========================================="
EOFDIAG
                
                chmod +x /tmp/diagnose_job_$$.sh
                
                JOB_ID=$(sbatch --nodes="$NODES" --gpus-per-node="$GPUS_PER_NODE" \
                    --ntasks-per-node=1 --parsable /tmp/diagnose_job_$$.sh)
                
                rm /tmp/diagnose_job_$$.sh
                
                echo "Job submitted: $JOB_ID"
                echo ""
                echo "Monitor with:"
                echo "  squeue -j $JOB_ID"
                echo "  tail -f logs/diagnose_${JOB_ID}.out"
                ;;
            standalone)
                echo "Running multi-node diagnostics with torchrun..."
                echo ""
                torchrun \
                    --nnodes="$NODES" \
                    --nproc_per_node="$GPUS_PER_NODE" \
                    src/gpu_diagnostics.py
                ;;
            *)
                echo "Multi-node diagnostics not supported for $env"
                echo "Running single-node diagnostics instead..."
                echo ""
                python src/gpu_diagnostics.py
                ;;
        esac
    fi
}

# Main command dispatcher
case $COMMAND in
    validate)
        run_validate
        ;;
    nccl)
        run_nccl
        ;;
    diagnose)
        run_diagnose
        ;;
    import)
        run_import
        ;;
    help|--help|-h)
        show_help
        ;;
    *)
        echo "Error: Unknown command: $COMMAND"
        echo ""
        echo "Available commands: validate, nccl, diagnose, import, help"
        echo "Use './gpu-test help' for more information"
        exit 1
        ;;
esac
