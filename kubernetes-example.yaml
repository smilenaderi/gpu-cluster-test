# GPU Cluster Acceptance Test - Kubernetes PyTorchJob Example
#
# This manifest deploys a distributed training job using Kubeflow's PyTorch Operator.
# Requires: PyTorch Operator installed in your cluster
#
# Usage:
#   kubectl apply -f kubernetes-example.yaml
#   kubectl get pytorchjobs
#   kubectl logs -f <pod-name>
#   kubectl delete -f kubernetes-example.yaml

apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: gpu-acceptance-test
  namespace: default
spec:
  # PyTorch distributed training configuration
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: gpu-acceptance-test
            role: master
        spec:
          # Init container to clone the repository
          initContainers:
            - name: git-clone
              image: alpine/git
              command:
                - sh
                - -c
                - |
                  git clone https://github.com/smilenaderi/gpu-cluster-test.git /workspace
              volumeMounts:
                - name: code
                  mountPath: /workspace
          containers:
            - name: pytorch
              image: pytorch/pytorch:2.4.0-cuda12.4-cudnn9-devel
              imagePullPolicy: IfNotPresent
              command:
                - "bash"
                - "-c"
                - |
                  pip install -q torchvision --no-deps && \
                  torchrun \
                    --nnodes=2 \
                    --nproc_per_node=8 \
                    --rdzv_backend=c10d \
                    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
                    /workspace/gpu-cluster-test/src/train.py --epochs 5 --batch-size 64
              resources:
                limits:
                  nvidia.com/gpu: 8
                  memory: "64Gi"
                  cpu: "16"
                requests:
                  nvidia.com/gpu: 8
                  memory: "32Gi"
                  cpu: "8"
              volumeMounts:
                - name: code
                  mountPath: /workspace
              env:
                - name: NCCL_DEBUG
                  value: "INFO"
          volumes:
            - name: code
              emptyDir: {}
          # Optional: Node affinity for GPU nodes
          # nodeSelector:
          #   accelerator: nvidia-gpu
    
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: gpu-acceptance-test
            role: worker
        spec:
          # Init container to clone the repository
          initContainers:
            - name: git-clone
              image: alpine/git
              command:
                - sh
                - -c
                - |
                  git clone https://github.com/smilenaderi/gpu-cluster-test.git /workspace
              volumeMounts:
                - name: code
                  mountPath: /workspace
          containers:
            - name: pytorch
              image: pytorch/pytorch:2.4.0-cuda12.4-cudnn9-devel
              imagePullPolicy: IfNotPresent
              command:
                - "bash"
                - "-c"
                - |
                  pip install -q torchvision --no-deps && \
                  torchrun \
                    --nnodes=2 \
                    --nproc_per_node=8 \
                    --rdzv_backend=c10d \
                    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
                    /workspace/gpu-cluster-test/src/train.py --epochs 5 --batch-size 64
              resources:
                limits:
                  nvidia.com/gpu: 8
                  memory: "64Gi"
                  cpu: "16"
                requests:
                  nvidia.com/gpu: 8
                  memory: "32Gi"
                  cpu: "8"
              volumeMounts:
                - name: code
                  mountPath: /workspace
              env:
                - name: NCCL_DEBUG
                  value: "INFO"
          volumes:
            - name: code
              emptyDir: {}
          # Optional: Node affinity for GPU nodes
          # nodeSelector:
          #   accelerator: nvidia-gpu
